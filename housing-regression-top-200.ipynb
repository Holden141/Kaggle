{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n# my imports\npd.set_option('display.max_rows', 100)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport missingno as msno\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-02T19:32:24.048234Z","iopub.execute_input":"2023-10-02T19:32:24.048571Z","iopub.status.idle":"2023-10-02T19:32:25.946320Z","shell.execute_reply.started":"2023-10-02T19:32:24.048543Z","shell.execute_reply":"2023-10-02T19:32:25.945193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")\n\ndef get_info(df: pd.DataFrame):\n    dtypes = df.dtypes.to_frame('Data Type') #remember each column by itself is just a series, we need it saved as a dataframe so we can customise the info output\n    non_null = df.count().to_frame('Non-Null Count') #anything less than 1460 is a NA value\n    unique_values = df.nunique().to_frame('Unique Count')\n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    min_value1 = df[numeric_cols].min().to_frame('Min Value')\n    max_value1 = df[numeric_cols].max().to_frame('Max Value')\n    info = pd.concat([dtypes, non_null, unique_values, min_value1, max_value1], axis=1) # Consolidate    \n    # Sorting the output is the whole reason i made this function\n    info = info.sort_values(['Data Type', info.columns[0]])\n    return info\n\nprint(train.shape)\n#get_info(train) #large output","metadata":{"execution":{"iopub.status.busy":"2023-10-02T19:32:25.948479Z","iopub.execute_input":"2023-10-02T19:32:25.949478Z","iopub.status.idle":"2023-10-02T19:32:26.025165Z","shell.execute_reply.started":"2023-10-02T19:32:25.949424Z","shell.execute_reply":"2023-10-02T19:32:26.024054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA & Feature Engineering\n\n**Id**  is useless as a feature and we can safely drop it. We also have to drop the SalePrice but i will keep it for now for factor analysis.","metadata":{}},{"cell_type":"code","source":"train.drop(['Id'], axis=1, inplace=True)\ntest.drop(['Id'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T19:32:26.026410Z","iopub.execute_input":"2023-10-02T19:32:26.026754Z","iopub.status.idle":"2023-10-02T19:32:26.045393Z","shell.execute_reply.started":"2023-10-02T19:32:26.026727Z","shell.execute_reply":"2023-10-02T19:32:26.044689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax = train.hist(bins=30, figsize=(50, 50))  #inspecting the distributions to get ideas on handling NAs/Outliers\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-02T19:32:26.047180Z","iopub.execute_input":"2023-10-02T19:32:26.047706Z","iopub.status.idle":"2023-10-02T19:32:34.539767Z","shell.execute_reply.started":"2023-10-02T19:32:26.047677Z","shell.execute_reply":"2023-10-02T19:32:34.538685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_num = train.select_dtypes(include = ['float64','int64'])\n#df_num.hist(figsize=(16, 20), bins=50, xlabelsize=8, ylabelsize=8); #numeric historgram\n# #correlation matrix\ncorrmat = df_num.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.9, square=True,cmap=\"crest\");\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T19:32:34.541085Z","iopub.execute_input":"2023-10-02T19:32:34.541692Z","iopub.status.idle":"2023-10-02T19:32:35.384739Z","shell.execute_reply.started":"2023-10-02T19:32:34.541657Z","shell.execute_reply":"2023-10-02T19:32:35.383680Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corrmat[\"SalePrice\"].sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T19:32:35.386325Z","iopub.execute_input":"2023-10-02T19:32:35.387011Z","iopub.status.idle":"2023-10-02T19:32:35.397510Z","shell.execute_reply.started":"2023-10-02T19:32:35.386972Z","shell.execute_reply":"2023-10-02T19:32:35.396718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The top 5 features (except SalePrice ofcourse) needs special attention. Going back to the distribution plots above now that we know how important GrLivArea is (2nd most important factor according to our matrix) it would be good to get rid of outliers.\n","metadata":{}},{"cell_type":"code","source":"train.drop(train[train[\"GrLivArea\"] > 4000].index, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T19:32:35.398949Z","iopub.execute_input":"2023-10-02T19:32:35.399581Z","iopub.status.idle":"2023-10-02T19:32:35.406753Z","shell.execute_reply.started":"2023-10-02T19:32:35.399544Z","shell.execute_reply":"2023-10-02T19:32:35.405992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets see our missing values.","metadata":{}},{"cell_type":"code","source":"msno.matrix(train, sort=\"descending\", figsize=(25, 10), width_ratios=(15, 1), color=(0.4, 0.7, 0.15), fontsize=10, labels=True, label_rotation=90, sparkline=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T19:32:35.407818Z","iopub.execute_input":"2023-10-02T19:32:35.408299Z","iopub.status.idle":"2023-10-02T19:32:37.091619Z","shell.execute_reply.started":"2023-10-02T19:32:35.408272Z","shell.execute_reply":"2023-10-02T19:32:37.090606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks like we are also gonna have to handle lots of missing values.\n","metadata":{}},{"cell_type":"markdown","source":"I use this part of the notebook for inspecting information about our attributes.","metadata":{}},{"cell_type":"code","source":"#for i in train.columns:\n#    count = train[i].nunique() \n#    print('Count of unique', i, 'is:', count)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T19:32:37.093180Z","iopub.execute_input":"2023-10-02T19:32:37.093782Z","iopub.status.idle":"2023-10-02T19:32:37.097944Z","shell.execute_reply.started":"2023-10-02T19:32:37.093746Z","shell.execute_reply":"2023-10-02T19:32:37.096984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#unique_values = train['PoolQC'].unique()\n#unique_values = train['Street'].unique()\n#print(train['Street'].value_counts())\n#print(train['PoolQC'].value_counts())\n#unique_values = train['Utilities'].unique()\n#print(train['Utilities'].value_counts())\nunique_values = train['PoolArea'].unique();print(unique_values) #\nprint(train['PoolArea'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2023-10-02T19:32:37.103049Z","iopub.execute_input":"2023-10-02T19:32:37.103466Z","iopub.status.idle":"2023-10-02T19:32:37.115492Z","shell.execute_reply.started":"2023-10-02T19:32:37.103419Z","shell.execute_reply":"2023-10-02T19:32:37.114444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train['Exterior2nd'].describe())\n#print(train['BsmtQual'].describe())","metadata":{"execution":{"iopub.status.busy":"2023-10-02T19:32:37.116911Z","iopub.execute_input":"2023-10-02T19:32:37.117360Z","iopub.status.idle":"2023-10-02T19:32:37.129299Z","shell.execute_reply.started":"2023-10-02T19:32:37.117322Z","shell.execute_reply":"2023-10-02T19:32:37.128469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['SalePrice'].hist(bins=100) #sale price is skewed","metadata":{"execution":{"iopub.status.busy":"2023-10-02T19:32:37.130691Z","iopub.execute_input":"2023-10-02T19:32:37.131244Z","iopub.status.idle":"2023-10-02T19:32:37.510735Z","shell.execute_reply.started":"2023-10-02T19:32:37.131208Z","shell.execute_reply":"2023-10-02T19:32:37.510011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n*log1p* will normalise our target as it has a left skew and is also a necessary transformation for our evaluation metric once we build our models (MSLE - Mean Squared Logarithmic Error). \n\nAt the time of evaluation, when we submit our model and check its performance on the leaderboard, we will have to use the inverseof log1p, *expm1* to transform the entries back to normal pricing values.","metadata":{}},{"cell_type":"code","source":"train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"]) ","metadata":{"execution":{"iopub.status.busy":"2023-10-02T19:32:37.511714Z","iopub.execute_input":"2023-10-02T19:32:37.512282Z","iopub.status.idle":"2023-10-02T19:32:37.517336Z","shell.execute_reply.started":"2023-10-02T19:32:37.512255Z","shell.execute_reply":"2023-10-02T19:32:37.516354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Its time to start proper feature engineering. \nI will save the target of the train data as *y* then concatenate the train and ","metadata":{}},{"cell_type":"code","source":"y = train['SalePrice'].reset_index(drop=True)\ntrain_features = train.drop(['SalePrice'], axis=1)\ntest_features = test\nfeatures = pd.concat([train_features, test_features]).reset_index(drop=True)\nfeatures.shape","metadata":{"execution":{"iopub.status.busy":"2023-10-02T19:32:37.518747Z","iopub.execute_input":"2023-10-02T19:32:37.519485Z","iopub.status.idle":"2023-10-02T19:32:37.542792Z","shell.execute_reply.started":"2023-10-02T19:32:37.519439Z","shell.execute_reply":"2023-10-02T19:32:37.541766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are gonna start with recastings and then imputations.","metadata":{}},{"cell_type":"code","source":"#Recasting these as strings instead of numbers\nfeatures['MSSubClass'] = features['MSSubClass'].apply(str)\nfeatures['YrSold'] = features['YrSold'].astype(str)\nfeatures['MoSold'] = features['MoSold'].astype(str)\n#dropping these \nfeatures = features.drop(['Utilities', 'Street', 'PoolQC',], axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T19:32:37.544036Z","iopub.execute_input":"2023-10-02T19:32:37.544503Z","iopub.status.idle":"2023-10-02T19:32:37.557792Z","shell.execute_reply.started":"2023-10-02T19:32:37.544477Z","shell.execute_reply":"2023-10-02T19:32:37.556680Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#mode imputes\nfeatures['Exterior1st'] = features['Exterior1st'].fillna(features['Exterior1st'].mode()[0]) \nfeatures['Exterior2nd'] = features['Exterior2nd'].fillna(features['Exterior2nd'].mode()[0])\nfeatures['SaleType'] = features['SaleType'].fillna(features['SaleType'].mode()[0])\nfeatures['Functional'] = features['Functional'].fillna(features['Functional'].mode()[0]) \nfeatures['Electrical'] = features['Electrical'].fillna(features['Electrical'].mode()[0])\nfeatures['KitchenQual'] = features['KitchenQual'].fillna(features['KitchenQual'].mode()[0])","metadata":{"execution":{"iopub.status.busy":"2023-10-02T19:32:37.559068Z","iopub.execute_input":"2023-10-02T19:32:37.559452Z","iopub.status.idle":"2023-10-02T19:32:37.577616Z","shell.execute_reply.started":"2023-10-02T19:32:37.559426Z","shell.execute_reply":"2023-10-02T19:32:37.576627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Zero, and None imputes\nfor col in (['GarageYrBlt', 'GarageArea', 'GarageCars']):\n    features[col] = features[col].fillna(0)\n    \nfor col in ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2','GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n    features[col] = features[col].fillna('None')","metadata":{"execution":{"iopub.status.busy":"2023-10-02T19:32:37.578860Z","iopub.execute_input":"2023-10-02T19:32:37.579498Z","iopub.status.idle":"2023-10-02T19:32:37.591873Z","shell.execute_reply.started":"2023-10-02T19:32:37.579469Z","shell.execute_reply":"2023-10-02T19:32:37.590958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Imputing MS zoning based on the MSSubclass \nfeatures['MSZoning'] = features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n\n#Imputing LotFrontage based on Neighborhood\nfeatures['LotFrontage'] = features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n\n#Imputing None to objects\nobjects = []\nfor i in features.columns:\n    if features[i].dtype == object:\n        objects.append(i)\nfeatures.update(features[objects].fillna('None'))\n\n#Imputing 0 to numerics\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerics = []\nfor i in features.columns:\n    if features[i].dtype in numeric_dtypes:\n        numerics.append(i)\nfeatures.update(features[numerics].fillna(0))","metadata":{"execution":{"iopub.status.busy":"2023-10-02T19:32:37.593178Z","iopub.execute_input":"2023-10-02T19:32:37.593439Z","iopub.status.idle":"2023-10-02T19:32:37.666221Z","shell.execute_reply.started":"2023-10-02T19:32:37.593417Z","shell.execute_reply":"2023-10-02T19:32:37.665131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With these features we want to emphasize, their existance (of a basement,pool, etc), in addition to their dimensions.\nBy the end will will onehot encode every categorical variable and finalise our features before training.","metadata":{}},{"cell_type":"code","source":"features['haspool'] = features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['has2ndfloor'] = features['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasgarage'] = features['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasbsmt'] = features['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasfireplace'] = features['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n#feature matrix projection\nfinal_features = pd.get_dummies(features).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T19:32:37.667543Z","iopub.execute_input":"2023-10-02T19:32:37.667886Z","iopub.status.idle":"2023-10-02T19:32:37.728438Z","shell.execute_reply.started":"2023-10-02T19:32:37.667859Z","shell.execute_reply":"2023-10-02T19:32:37.727699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will split the features in two, the first part will select the rows of the training set according to the length of y, and the rest will be the test set. ","metadata":{}},{"cell_type":"code","source":"X = final_features.iloc[:len(y), :]\t\nX_sub = final_features.iloc[len(y):, :]","metadata":{"execution":{"iopub.status.busy":"2023-10-02T19:32:37.729549Z","iopub.execute_input":"2023-10-02T19:32:37.730550Z","iopub.status.idle":"2023-10-02T19:32:37.736755Z","shell.execute_reply.started":"2023-10-02T19:32:37.730516Z","shell.execute_reply":"2023-10-02T19:32:37.735587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"markdown","source":"The most commonly used strategy is to blend regressors and average their results. ","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom mlxtend.regressor import StackingCVRegressor\nfrom xgboost import XGBRegressor\nfrom datetime import datetime","metadata":{"execution":{"iopub.status.busy":"2023-10-02T20:13:30.628019Z","iopub.execute_input":"2023-10-02T20:13:30.628373Z","iopub.status.idle":"2023-10-02T20:13:30.633684Z","shell.execute_reply.started":"2023-10-02T20:13:30.628346Z","shell.execute_reply":"2023-10-02T20:13:30.632685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we will define are cross validation strategy (5 folds), the evaluation metric (RMSLE) for the models and for their folds (cv_rmse) and the models themselves.\nXGBoostRegressor would be the first candidate but since we are stacking, we will also add its slightly less regularized brother, GradientBoostingRegressor and also Ridge and Lasso regressors, as their are strong regularisers and can help prevent overfitting as well as elasticnet which is kind of a combination of the two and combine everything in the StackingRegressors.\nSince we will be crossvalidating every model, we will use their CV counterparts.\n\nThis is the most computationally demanding part of the analysis, as we have an infinite amount of values to optimise. I will be using some defaults and values other people have had success with.\nThe most obvious ones are the strength of the regularisation (L1,L2) for lasso ,ridge, elasticnet which is their alpha value. Elasticnet uses a ratio of the two called l1_ratio. \n\nThe boosters can optimise their learning_rate, number of estimators, max depth of each etc~.\nXGBoost will be also the model that will be taking the votes of every other model at the end.","metadata":{}},{"cell_type":"markdown","source":"I will be printing out the score of each model, in order to assign a weight to its performance when its time to blend the models.","metadata":{}},{"cell_type":"code","source":"kfolds = KFold(n_splits=5, shuffle=True, random_state=42)\n\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\ndef cv_rmse(model, X=X):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kfolds))\n    return (rmse)\n\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\n\nridge = make_pipeline(RobustScaler(), RidgeCV(cv=kfolds))\nlasso = make_pipeline(RobustScaler(), LassoCV(random_state=42, cv=kfolds))\nelasticnet= make_pipeline(RobustScaler(), ElasticNetCV(cv=kfolds, l1_ratio=e_l1ratio))\n\ngbr = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                max_depth=4, max_features='sqrt', min_samples_leaf=15, min_samples_split=10, \n                                loss='huber', random_state =42)\n#i am gonna double the estimators i used in a previous run and also double the max_depth\n#also gonna try higher learning rate\nxgboost = XGBRegressor(learning_rate=0.05, n_estimators=7200,\n                                     max_depth=6, min_child_weight=0,\n                                     gamma=0, subsample=0.7,\n                                     colsample_bytree=0.2,\n                                     objective='reg:squarederror', nthread=-1, #reg:linear is deprecated\n                                     scale_pos_weight=1, seed=27,\n                                     reg_alpha=0.00006)\n\nstack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet, gbr, xgboost,),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True)\n\nscore = cv_rmse(ridge)\nprint(\"Ridge: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()),datetime.now(),)\nscore = cv_rmse(gbr)\nprint(\"gbr: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()),datetime.now(),)\nscore = cv_rmse(xgboost)\nprint(\"xgboost: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()),datetime.now(),)\nscore = cv_rmse(lasso)\nprint(\"lasso:: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\nscore = cv_rmse(elasticnet)\nprint(\"elasticnet:: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\n\nridge_model_full_data = ridge.fit(X, y)\ngbr_model_full_data = gbr.fit(X, y)\nxgb_model_full_data = xgboost.fit(X, y)\nlasso_model_full_data = lasso.fit(X, y)\nelasticnet_model_full_data = elasticnet.fit(X,y)\nstack_gen_model = stack_gen.fit(np.array(X), np.array(y))","metadata":{"execution":{"iopub.status.busy":"2023-10-02T19:56:13.662409Z","iopub.execute_input":"2023-10-02T19:56:13.662830Z","iopub.status.idle":"2023-10-02T20:02:46.113111Z","shell.execute_reply.started":"2023-10-02T19:56:13.662797Z","shell.execute_reply":"2023-10-02T20:02:46.112033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def blend_models_predict(X):\n    return ((0.2 * ridge_model_full_data.predict(X)) + \\\n            (0.1 * gbr_model_full_data.predict(X)) + \\\n            (0.1 * lasso_model_full_data.predict(X)) + \\\n            (0.1 * elasticnet_model_full_data.predict(X)) + \\\n            (0.1 * xgb_model_full_data.predict(X)) + \\\n            (0.4 * stack_gen_model.predict(np.array(X))))\n\nprint(rmsle(y, blend_models_predict(X)))","metadata":{"execution":{"iopub.status.busy":"2023-10-02T20:02:46.115421Z","iopub.execute_input":"2023-10-02T20:02:46.115747Z","iopub.status.idle":"2023-10-02T20:02:47.032456Z","shell.execute_reply.started":"2023-10-02T20:02:46.115720Z","shell.execute_reply":"2023-10-02T20:02:47.031702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/sample_submission.csv\")\nsubmission.iloc[:,1] = np.floor(np.expm1(blend_models_predict(X_sub)))\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T19:39:58.763552Z","iopub.execute_input":"2023-10-02T19:39:58.764158Z","iopub.status.idle":"2023-10-02T19:39:59.673987Z","shell.execute_reply.started":"2023-10-02T19:39:58.764126Z","shell.execute_reply":"2023-10-02T19:39:59.673151Z"},"trusted":true},"execution_count":null,"outputs":[]}]}